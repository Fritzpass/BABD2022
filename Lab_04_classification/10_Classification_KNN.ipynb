{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassification: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "#any further infos on the diabetes dataset:\n",
    "#https://www.kaggle.com/uciml/pima-indians-diabetes-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "- Pregnancies:  Number of times pregnant\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "- SkinThickness: Triceps skin fold thickness (mm)\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "- DiabetesPedigreeFunction: Diabetes pedigree function\n",
    "- Age: (years)\n",
    "- Outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives information about the data types,columns, null value counts, memory usage etc\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic statistic details about the data\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print class freq. through pandas: we group the data by the column target and we count the number of rows \n",
    "target_dist=df.groupby('target').size()\n",
    "print(target_dist)\n",
    "\n",
    "#some imports to plot \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Visualize Class Counts\n",
    "target_dist.plot.bar(x='',y='',title='Distribution of target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Train/Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate X and y (explanatory variables and target variable)\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1] #[-1]]\n",
    "\n",
    "#X.head()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#SPLIT DATA INTO TRAIN AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size =0.30, #by default is 75%-25%\n",
    "                                                    #shuffle is set True by default,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state= 123\n",
    "                                                   ) #fix random seed for replicability\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dist=y_train.groupby(y_train.iloc[:]).size()/y_train.size\n",
    "y_test_dist=y_test.groupby(y_test.iloc[:]).size()/y_test.size\n",
    "\n",
    "train_test_dist = pd.DataFrame({'train': y_train_dist, 'test': y_test_dist})\n",
    "ax = train_test_dist.plot.bar(rot=0) # rotation of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = X_train.iloc[:,2]\n",
    "y = X_train.iloc[:,6]\n",
    "\n",
    "plt.scatter(x, y,alpha=0.2,c=y_train )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first model: K-Nearest Neighbor\n",
    "\n",
    "The K-Nearest Neighbor classification model simply assign to new observation the most common value among its k-nearest neighbors\n",
    "\n",
    "![KNN](KnnClassification.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE YOUR CLASSIFIER and THE PARAMETERS GRID\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn40 = KNeighborsClassifier(n_neighbors=40)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn40.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = knn40.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE OUR PREDICTION\n",
    "from sklearn import metrics\n",
    "\n",
    "print('***RESULTS ON TEST SET***')\n",
    "print(\"F1_score: \",metrics.f1_score(y_test, y_pred))\n",
    "print(\"Accuracy\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Recall\",metrics.recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINE YOUR GRIDSEARCH \n",
    "\n",
    "GS perfoms an exhaustive search over specified parameter values for an estimator.\n",
    "GS uses a Stratified K-Folds cross-validator\n",
    "(The folds are made by preserving the percentage of samples for each class.)\n",
    "\n",
    "Some Parameters:\n",
    "\n",
    "- estimator : estimator object.\n",
    "- param_grid : dict or list of dictionaries\n",
    "- scoring : scoring parameter\n",
    "- n_jobs : Number of jobs to run in parallel. -1 means using all processors\n",
    "- cv : cross-validation generator (default 3-fold cross validation)\n",
    "- verbose : Controls the verbosity: the higher, the more messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Create KNN Classifier\n",
    "classifier = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':np.arange(1,40)}\n",
    "\n",
    "gs = GridSearchCV(classifier, parameters, cv=3, scoring = 'f1', verbose=90, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN YOUR CLASSIFIER\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize the results of your GRIDSEARCH\n",
    "print(\"Best score: %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "\n",
    "means = gs.cv_results_['mean_test_score']\n",
    "stds = gs.cv_results_['std_test_score']\n",
    "params = gs.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean %f Std (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot (means, color='blue', alpha=1.00)\n",
    "plt.show()\n",
    "\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON YOUR TEST SET \n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is your prediction on the TEST SET\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE YOUR PREDICTION IN THE TRAINING SET\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "\n",
    "print('***RESULTS ON TRAIN SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_train, y_pred_train)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_train, y_pred_train)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_train, y_pred_train)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_train, y_pred_train)) # (tp+tn)/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE YOUR PREDICTION IN THE TEST SET\n",
    "from sklearn import metrics \n",
    "\n",
    "print('***RESULTS ON TEST SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_test, y_pred)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_test, y_pred)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_test, y_pred)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred)) # (tp+tn)/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT SOME FURTHER METRICS\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\"); #annot=True to annotate cells fmt: format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=7)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_probs = model.predict_proba(X_test) #predict_proba gives the probabilities for the target (0 and 1 in your case) \n",
    "\n",
    "fpr, tpr, thresholds=metrics.roc_curve(y_test,  y_probs[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train=[]\n",
    "score_test=[]\n",
    "\n",
    "neighbors=range(1,180,7)\n",
    "\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "    score_train.append( metrics.f1_score(y_train, y_pred_train))\n",
    "    score_test.append( metrics.f1_score(y_test, y_pred_test))\n",
    "\n",
    "plt.xlabel('Neighbors')\n",
    "plt.ylabel('F1')\n",
    "plt.plot (neighbors,score_train, color='blue', alpha=1.00)\n",
    "plt.plot (neighbors,score_test, color='red', alpha=1.00)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot (neighbors,score_train, color='blue', alpha=1.00)\n",
    "plt.plot (neighbors,score_test, color='red', alpha=1.00)\n",
    "plt.xlabel('Neighbors (complexity)')\n",
    "plt.ylabel('F1 (error)')\n",
    "plt.xlim(175,0)\n",
    "plt.ylim(1,-0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=90)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE YOUR PREDICTION\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('***RESULTS ON TRAIN SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_train, y_pred_train)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_train, y_pred_train)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_train, y_pred_train)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_train, y_pred_train)) # (tp+tn)/m\n",
    "print(\"---\")\n",
    "print('***RESULTS ON TEST SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_test, y_pred)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_test, y_pred)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_test, y_pred)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred)) # (tp+tn)/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Reds\"); #annot=True to annotate cells fmt: format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict_proba(X_test) #predict_proba gives the probabilities for the target (0 and 1 in your case) \n",
    "\n",
    "fpr, tpr, thresholds=metrics.roc_curve(y_test,  y_probs[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "# We compute the scaler\n",
    "scaled_data = scaler.transform(X.astype(float))\n",
    "scaled_X = pd.DataFrame(scaled_data.astype(float))\n",
    "scaled_X.columns = X.columns\n",
    "\n",
    "# We apply the same scaler to the data \n",
    "scaled_data = scaler.transform(X_train.astype(float))\n",
    "scaled_X_train = pd.DataFrame(scaled_data.astype(float))\n",
    "scaled_X_train.columns = X_train.columns\n",
    "\n",
    "scaled_data = scaler.transform(X_test.astype(float))\n",
    "scaled_X_test = pd.DataFrame(scaled_data.astype(float))\n",
    "scaled_X_test.columns = X_test.columns\n",
    "\n",
    "scaled_X.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE  PARAMETERS GRID\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "classifier = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':np.arange(10,150)} # WE BEGIN FROM 10 TO PREVENT OVERFITTING\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(classifier, parameters, cv=3, scoring = 'f1', verbose=50, n_jobs=-1)\n",
    "gs = gs.fit(scaled_X_train, y_train)\n",
    "\n",
    "#Prediction\n",
    "print(\"Best score: %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(scaled_X_test)\n",
    "\n",
    "\n",
    "print(\"f1_score: \", metrics.f1_score(y_test, y_pred))\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\"); #annot=True to annotate cells fmt: format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=15)\n",
    "model.fit(scaled_X_train, y_train)\n",
    "y_pred = model.predict(scaled_X_test)\n",
    "\n",
    "y_probs = model.predict_proba(scaled_X_test) #predict_proba gives the probabilities for the target (0 and 1 in your case) \n",
    "\n",
    "fpr, tpr, thresholds=metrics.roc_curve(y_test,  y_probs[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_model.predict(scaled_X_train)\n",
    "\n",
    "print('***RESULTS ON TRAIN SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_train, y_pred_train)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_train, y_pred_train)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_train, y_pred_train)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_train, y_pred_train)) # (tp+tn)/m\n",
    "print('---')\n",
    "print('***RESULTS ON TEST SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_test, y_pred)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_test, y_pred)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_test, y_pred)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred)) # (tp+tn)/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train=[]\n",
    "score_test=[]\n",
    "\n",
    "neighbors=range(1,28,1)\n",
    "\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(scaled_X_train,y_train)\n",
    "    y_pred_train = knn.predict(scaled_X_train)\n",
    "    y_pred_test = knn.predict(scaled_X_test)\n",
    "    score_train.append( metrics.f1_score(y_train, y_pred_train))\n",
    "    score_test.append( metrics.f1_score(y_test, y_pred_test))\n",
    "\n",
    "plt.xlabel('Neighbors')\n",
    "plt.ylabel('F1')\n",
    "plt.plot (neighbors,score_train, color='blue', alpha=1.00)\n",
    "plt.plot (neighbors,score_test, color='red', alpha=1.00)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA fit\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_X)\n",
    "df_pca = pd.DataFrame(pca.transform(scaled_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance=pd.DataFrame(pca.explained_variance_ratio_)\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "ax = sns.barplot( data=explained_variance.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pd.DataFrame(data = pca.transform(scaled_X)\n",
    "             ,columns = ['pc1', 'pc2','pc3','pc4','pc5', 'pc6','pc7','pc8'])\n",
    "\n",
    "X_train_pca = pd.DataFrame(data = pca.transform(scaled_X_train)\n",
    "             ,columns = ['pc1', 'pc2','pc3','pc4','pc5', 'pc6','pc7','pc8'])\n",
    "\n",
    "X_test_pca = pd.DataFrame(data = pca.transform(scaled_X_test)\n",
    "             ,columns = ['pc1', 'pc2','pc3','pc4','pc5', 'pc6','pc7','pc8'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = X_train_pca.iloc[:,0]\n",
    "y = X_train_pca.iloc[:,1]\n",
    "\n",
    "plt.scatter(x, y,alpha=0.2,c=y_train )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = X_test_pca.iloc[:,0]\n",
    "y = X_test_pca.iloc[:,1]\n",
    "\n",
    "plt.scatter(x, y,alpha=0.2,c=y_test )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using two PC's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE  PARAMETERS GRID\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "classifier = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':np.arange(10,100)}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(classifier, parameters, cv=5, scoring = 'f1', verbose=50, n_jobs=-1)\n",
    "# We use just the two first PC\n",
    "gs = gs.fit(X_train_pca.iloc[:,:2], y_train) \n",
    "\n",
    "#Prediction\n",
    "print(\"Best score: %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test_pca.iloc[:,:2])\n",
    "\n",
    "print(\"f1_score: \", metrics.f1_score(y_test, y_pred))\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\"); #annot=True to annotate cells fmt: format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_model.predict(X_train_pca.iloc[:,:2])\n",
    "y_pred = best_model.predict(X_test_pca.iloc[:,:2])\n",
    "\n",
    "\n",
    "print('***RESULTS ON TRAIN SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_train, y_pred_train)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_train, y_pred_train)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_train, y_pred_train)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_train, y_pred_train)) # (tp+tn)/m\n",
    "print('---')\n",
    "print('***RESULTS ON TEST SET***')\n",
    "print(\"precision: \", metrics.precision_score(y_test, y_pred)) # tp / (tp + fp)\n",
    "print(\"recall: \", metrics.recall_score(y_test, y_pred)) # tp / (tp + fn)\n",
    "print(\"f1_score: \", metrics.f1_score(y_test, y_pred)) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred)) # (tp+tn)/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the three first PC\n",
    "X_train_pca_2=X_train_pca.iloc[:,:3]\n",
    "X_test_pca_2=X_test_pca.iloc[:,:3]\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=27)\n",
    "model.fit(X_train_pca_2, y_train)\n",
    "y_pred = model.predict(X_test_pca_2)\n",
    "\n",
    "y_probs = model.predict_proba(X_test_pca_2) #predict_proba gives the probabilities for the target (0 and 1 in your case) \n",
    "\n",
    "fpr, tpr, thresholds=metrics.roc_curve(y_test,  y_probs[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all the PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=15)\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "#Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\"); #annot=True to annotate cells fmt: format\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC\n",
    "y_probs = model.predict_proba(X_test_pca) #predict_proba gives the probabilities for the target (0 and 1 in your case) \n",
    "\n",
    "fpr, tpr, thresholds=metrics.roc_curve(y_test,  y_probs[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
