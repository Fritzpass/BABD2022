{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Multi Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boston Housing Dataset: Load the boston dataset.\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_boston = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df_boston['target'] = boston.target\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#sns.pairplot(df_boston, x_vars=boston.feature_names, y_vars='target')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 5,figsize=[15,8],constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "i=0\n",
    "for x in df_boston.columns[:-1]:\n",
    "    plt.sca(axes[i]) # set the current Axes\n",
    "    plt.scatter(df_boston[x],df_boston.target)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(\"target\")\n",
    "    i+=1\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call regplot on each axes\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True,figsize=[8,4])\n",
    "sns.scatterplot(x=df_boston['RM'], y=df_boston.target, ax=ax1)\n",
    "sns.scatterplot(x=df_boston['LSTAT'], y=df_boston.target, ax=ax2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "log_LSTAT=np.log(df_boston['LSTAT'])\n",
    "\n",
    "log_CRIM=np.log(df_boston['CRIM'])\n",
    "\n",
    "# call regplot on each axes\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True,figsize=[8,4])\n",
    "sns.scatterplot(x=df_boston['LSTAT'], y=df_boston.target, ax=ax1)\n",
    "sns.scatterplot(x=log_LSTAT, y=df_boston.target, ax=ax2)\n",
    "#sns.scatterplot(x=log_CRIM, y=df_boston.target, ax=ax1)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_boston, x='target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can compute the correlation \n",
    "df_boston.corr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#We can visualise the correlation using a heatmap in Seaborn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(data=df_boston.corr().round(2), cmap='coolwarm', linewidths=.5, annot=True, annot_kws={\"size\":12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From this we can see that the two variables with the strongest correlation to MEDV are:\n",
    "    1) the LSTAT(% lower status of the population)\n",
    "    2) RM (average number of rooms per dwelling).\n",
    "\n",
    "We can also use the heatmap to check for correlation between variables so that:\n",
    "    1) we don’t include multicollinearity into a linear regression. \n",
    "\n",
    "For example both RAD and TAX are highly correlated (0.91) so these both shouldn’t be used in a linear regression at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Brief discussion on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_q = pd.read_csv(\"quadratic.csv\") \n",
    "\n",
    "df_q.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df_q[\"x\"], df_q[\"y\"], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "df_q[\"x2\"]=df_q[\"x\"].apply(lambda x: -(x-0.5)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "simple_regr=LinearRegression()\n",
    "simple_regr.fit(df_q[[\"x\",\"x2\"]], df_q[\"y\"])\n",
    "\n",
    "y_pred=simple_regr.predict(df_q[[\"x\",\"x2\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_q[\"y\"],y_pred,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"r2: \",metrics.r2_score(df_q[\"y\"], y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Separate features and target variables\n",
    "X = df_boston.iloc[:,:-1] #if I want to use all variables\n",
    "y = df_boston.iloc[:,-1]\n",
    "\n",
    "#choose your approach:\n",
    "#X = df_boston.iloc[:,[4,5,10,11,12]] #if I want to use only some variable\n",
    "#X = X.drop(['INDUS','CHAS','AGE','B'], axis=1) #if I want to drop some columns\n",
    "#X = X[['RM','LSTAT']]#if I want to select some columns\n",
    "X[\"LSTAT2\"] = np.log(X.LSTAT) #if I want to log transform the LSTAT variable \n",
    "#X[\"CRIM2\"] = np.log(X.CRIM) #if I want to log transform the LSTAT variable \n",
    "\n",
    "columns = X.columns #column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[['RM','LSTAT2']]\n",
    "\n",
    "# Filter the unusual observation\n",
    "#X=X[y<50]\n",
    "#y=y[y<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale and select Train/Test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=False).fit(X)\n",
    "scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEFINE YOUR REGRESSOR and THE PARAMETERS GRID\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "regressor = LinearRegression() #(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "parameters = {}\n",
    "\n",
    "#DEFINE YOUR GRIDSEARCH \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(regressor, parameters,cv=3) #with no params it reduces to a CV\n",
    "\n",
    "gs = gs.fit(X_train,y_train)\n",
    "\n",
    "#summarize the results of your GRIDSEARCH\n",
    "print('***GRIDSEARCH RESULTS***')\n",
    "print(\"Best score: %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "means = gs.cv_results_['mean_test_score']\n",
    "stds = gs.cv_results_['std_test_score']\n",
    "params = gs.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "#test on hold-out\n",
    "\n",
    "#gs.score(X_train, y_train)\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, gs.predict(X_test))\n",
    "plt.xlabel(\"Prices: $y_i$\")\n",
    "plt.ylabel(\"Predicted prices: $\\hat{y}_i$\")\n",
    "plt.title(\"Prices vs Predicted prices: $y_i$ vs $\\hat{y}_i$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent term in the linear model.\n",
    "print('Intercept: ', gs.best_estimator_.intercept_)\n",
    "\n",
    "gs.best_estimator_.coef_\n",
    "\n",
    "#import pandas as pd\n",
    "#pd.DataFrame(list(zip(columns,gs.best_estimator_.coef_)), columns = ['features','estimatedCoefficients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE train: \", mean_squared_error(y_train, gs.predict(X_train)))\n",
    "print(\"MSE test: \", mean_squared_error(y_test, gs.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing MAE, MSE, RMSE, r²\n",
    " - Mean Absolute Error (MAE): $$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    " - Mean Squared Error  (MSE): $$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    " - Root Mean Squared Error (RMSE) : $$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"MAE train: \", metrics.mean_absolute_error(y_train, gs.predict(X_train))) \n",
    "print(\"MSE train: \",metrics.mean_squared_error(y_train, gs.predict(X_train)))\n",
    "print(\"RMSE train: \",np.sqrt(metrics.mean_squared_error(y_train, gs.predict(X_train))))\n",
    "print(\"r2: \",metrics.r2_score(y_train, gs.predict(X_train)))\n",
    "\n",
    "print(\"MAE test: \", metrics.mean_absolute_error(y_test, gs.predict(X_test))) \n",
    "print(\"MSE test: \",metrics.mean_squared_error(y_test, gs.predict(X_test)))\n",
    "print(\"RMSE test: \",np.sqrt(metrics.mean_squared_error(y_test, gs.predict(X_test))))\n",
    "print(\"r2: \",metrics.r2_score(y_test, gs.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train=gs.predict(X_train)-y_train\n",
    "error_test=gs.predict(X_test)-y_test\n",
    "\n",
    "error_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gs.predict(X_train),error_train, c=\"b\", label=\"training data\")\n",
    "plt.scatter(gs.predict(X_test),error_test, c=\"g\", label=\"test data\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color=\"r\")\n",
    "plt.xlim([-10,50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_train = np.array(error_train).flatten()\n",
    "\n",
    "error_train = np.array(error_train).reshape(-1,1)\n",
    "scaled_error_train= StandardScaler(copy=False).fit(error_train).transform(error_train).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import qqplot_2samples\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# We test a exponential distribution\n",
    "dist = getattr(scipy.stats, 'norm')\n",
    "param = dist.fit(nb_error_train)\n",
    "\n",
    "err_mean=param[-2]\n",
    "err_std=param[-1]\n",
    "\n",
    "# We generate a sample of size  len(mr_scaled) of data distributed according to distribution dist\n",
    "# The function rvs generates a sample with distribution dist with mean loc and std scale\n",
    "test_dist = dist.rvs(*param[0:-2],loc=param[-2], scale=param[-1],size = len(nb_error_train))\n",
    "\n",
    "# qq-plot using statsmodels\n",
    "qqplot_2samples(test_dist,np.array(nb_error_train).flatten(), line='45')\n",
    "plt.show()\n",
    "\n",
    "# We create the percentiles for both distributions\n",
    "test_dist.sort()\n",
    "percs = np.linspace(0,100,21)\n",
    "q_a = np.percentile(nb_error_train, percs)\n",
    "q_b = np.percentile(test_dist, percs)\n",
    "\n",
    "# and generate the QQ-plot \n",
    "plt.plot(q_a,q_b, ls=\"\", marker=\"o\")\n",
    "plt.title(\"QQ plot\")\n",
    "x = np.linspace(np.min((q_a.min(),q_b.min())), np.max((q_a.max(),q_b.max())))\n",
    "plt.plot(x,x, color=\"k\", ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the distribution and compare with a normal\n",
    "\n",
    "ax = sns.histplot(nb_error_train, stat='density')\n",
    "\n",
    "# calculate the pdf\n",
    "x0, x1 = ax.get_xlim()  # extract the endpoints for the x-axis\n",
    "x_pdf = np.linspace(x0, x1, 100)\n",
    "y_pdf = scipy.stats.norm.pdf(x_pdf, loc=err_mean, scale=err_std)\n",
    "\n",
    "ax.plot(x_pdf, y_pdf, 'r', lw=2, label='normal')                                                   \n",
    "ax.legend() \n",
    "\n",
    "#plt.hist(nb_error_train,alpha=.3, density=True,bins='auto')\n",
    "#plt.hist(test_dist,alpha=.3, density=True,bins='auto')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov Test\n",
    "from scipy import stats\n",
    "print(stats.kstest(scaled_error_train, \"norm\"))\n",
    "print(stats.kstest(nb_error_train, test_dist))\n",
    "# normality tests use a (0,1) normal distribution \n",
    "# D'agostino normality test\n",
    "print(stats.normaltest(scaled_error_train))\n",
    "# Shapiro test of normality\n",
    "print(stats.shapiro(scaled_error_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parameters (statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "X_train = sm.add_constant(X_train)\n",
    "#If we want to add a constant to our model \n",
    "est = sm.OLS(y_train, X_train)\n",
    "est_fit = est.fit()\n",
    "est_fit.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est_fit.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
