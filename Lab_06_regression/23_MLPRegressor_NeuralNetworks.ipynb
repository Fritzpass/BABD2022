{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron Regression\n",
    "\n",
    "Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Boston Housing Dataset: Load the boston dataset.\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "#Creating feature and target arrays\n",
    "X, y = boston.data, boston.target\n",
    "columns = boston.feature_names\n",
    "\n",
    "X=X[y<50]\n",
    "y=y[y<50]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=False).fit(X)\n",
    "scaler.transform(X)\n",
    "\n",
    "'''\n",
    "Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. \n",
    "For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], \n",
    "or standardize it to have mean 0 and variance 1. Note that you must apply the same scaling \n",
    "to the test set for meaningful results. You can use StandardScaler for standardization.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "#DEFINE YOUR REGRESSOR and THE PARAMETERS GRID\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "regressor = MLPRegressor(random_state=0)\n",
    "parameters = {'hidden_layer_sizes': [(8),(10, 5), (20,10,5), (10,5,3)],\n",
    "              'solver' : ['sgd'],\n",
    "              #'batch_size': [20],\n",
    "              #'learning_rate' : ['constant'],\n",
    "              'alpha':10.0 ** -np.arange(-1, 3),\n",
    "              'max_iter':[10000]}\n",
    "\n",
    "#DEFINE YOUR GRIDSEARCH \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(regressor, parameters, cv=3, verbose = 0) #with no params it reduces to a CV\n",
    "\n",
    "gs = gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize the results of your GRIDSEARCH\n",
    "print('***GRIDSEARCH RESULTS***')\n",
    "print(\"Best score: %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "means = gs.cv_results_['mean_test_score']\n",
    "stds = gs.cv_results_['std_test_score']\n",
    "params = gs.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "#Explained variance score: 1 is perfect prediction\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(gs.predict(X_train),gs.predict(X_train)-y_train, c=\"b\", label=\"training data\")\n",
    "plt.scatter(gs.predict(X_test),gs.predict(X_test)-y_test, c=\"g\", label=\"test data\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color=\"r\")\n",
    "plt.xlim([-10,50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"MAE train: \", metrics.mean_absolute_error(y_train, gs.predict(X_train))) \n",
    "print(\"MSE train: \",metrics.mean_squared_error(y_train, gs.predict(X_train)))\n",
    "print(\"RMSE train: \",np.sqrt(metrics.mean_squared_error(y_train, gs.predict(X_train))))\n",
    "print(\"r2: \",np.sqrt(metrics.r2_score(y_train, gs.predict(X_train))))\n",
    "\n",
    "print(\"MAE test: \", metrics.mean_absolute_error(y_test, gs.predict(X_test))) \n",
    "print(\"MSE test: \",metrics.mean_squared_error(y_test, gs.predict(X_test)))\n",
    "print(\"RMSE test: \",np.sqrt(metrics.mean_squared_error(y_test, gs.predict(X_test))))\n",
    "print(\"r2: \",np.sqrt(metrics.r2_score(y_test, gs.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "error_train=gs.predict(X_train)-y_train\n",
    "error_test=gs.predict(X_test)-y_test\n",
    "\n",
    "pd.DataFrame(error_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_train = np.array(error_train).flatten()\n",
    "\n",
    "error_train = np.array(error_train).reshape(-1,1)\n",
    "scaled_error_train= StandardScaler(copy=False).fit(error_train).transform(error_train).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import qqplot_2samples\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# We test a exponential distribution\n",
    "dist = getattr(scipy.stats, 'norm')\n",
    "param = dist.fit(nb_error_train)\n",
    "\n",
    "err_mean=param[-2]\n",
    "err_std=param[-1]\n",
    "\n",
    "# We generate a sample of size  len(mr_scaled) of data distributed according to distribution dist\n",
    "# The function rvs generates a sample with distribution dist with mean loc and std scale\n",
    "test_dist = dist.rvs(*param[0:-2],loc=param[-2], scale=param[-1],size = len(nb_error_train))\n",
    "\n",
    "\n",
    "# plot the distribution and compare with a normal\n",
    "\n",
    "ax = sns.histplot(nb_error_train, stat='density')\n",
    "\n",
    "# calculate the pdf\n",
    "x0, x1 = ax.get_xlim()  # extract the endpoints for the x-axis\n",
    "x_pdf = np.linspace(x0, x1, 100)\n",
    "y_pdf = scipy.stats.norm.pdf(x_pdf, loc=err_mean, scale=err_std)\n",
    "\n",
    "ax.plot(x_pdf, y_pdf, 'r', lw=2, label='normal')                                                   \n",
    "ax.legend() \n",
    "\n",
    "#plt.hist(nb_error_train,alpha=.3, density=True,bins='auto')\n",
    "#plt.hist(test_dist,alpha=.3, density=True,bins='auto')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
